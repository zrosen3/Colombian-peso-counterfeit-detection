{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #For plotting our visualizations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(image_size: tuple = (480, 640), seed: int = 1234, ds_num: int = 1) -> tuple:\n",
    "    \"\"\"\n",
    "    Load all the images in the given dataset folder. Since all the images in the\n",
    "    dataset are already split in separate folders, this function\n",
    "    will extract each and return them as a tuple of `tf.data.Dataset`, along with\n",
    "    a list of the class names.\n",
    "\n",
    "    Args:\n",
    "        image_size: size the images will be processed into (h,w) \n",
    "            (default = (480,640))\n",
    "        seed: random seed to shuffle the dataset with. Use `None` if randomizing\n",
    "            is not required. (default = 1234)\n",
    "        ds_num: the dataset number corrisponding to the folder to extract the\n",
    "            dataset from. (ex. 1 = \"ds1\") (default = 1)\n",
    "    \n",
    "    Returns:\n",
    "        `tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, list[string]]`\n",
    "        where it is (Train, Test, Validation, and class names) respectively\n",
    "    \"\"\"\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        f\"../Data/Original/ds{ds_num}/Train/\",\n",
    "        image_size=image_size,\n",
    "        seed=seed,\n",
    "    )\n",
    "    test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        f\"../Data/Original/ds{ds_num}/Test/\",\n",
    "        image_size=image_size,\n",
    "        seed=seed,\n",
    "    )\n",
    "    validation_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        f\"../Data/Original/ds{ds_num}/Validation/\",\n",
    "        image_size=image_size,\n",
    "        seed=seed,\n",
    "    )\n",
    "    return (train_ds, test_ds, validation_ds, train_ds.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PeakData(dataset: tf.data.Dataset,\n",
    "             class_names: list,\n",
    "             nrows: int = 3,\n",
    "             ncols: int = 3,\n",
    "             prediction_labels: list = None\n",
    "            ) -> None:\n",
    "    \"\"\"\n",
    "    Displays the images in the given dataset. If predictions are given, it will\n",
    "    say in the title what the prediction was vs the what it actually is.\n",
    "\n",
    "    Args:\n",
    "        dataset: the dataset to view some images from\n",
    "        class_names: list of the names of the classifications\n",
    "        nrows: number of rows to display (default = 3)\n",
    "        ncols: number of columns to display (default = 3)\n",
    "        prediction_labels: list of the predictions. If None, it won't be used\n",
    "            (default = None)\n",
    "    \"\"\"\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(nrows * ncols):\n",
    "            plt.subplot(nrows, ncols, i + 1)\n",
    "            plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title(class_names[labels[i]] if prediction_labels == None else f\"pred: {class_names[prediction_labels[i]]} | actual: {class_names[labels[i]]}\")\n",
    "            plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeModel(class_names: list) -> tf.keras.Sequential:\n",
    "    \"\"\"\n",
    "    Simple straight forward CNN model. this is just for simplicity and testing\n",
    "    atm. I will make it more modular later once I know what we are doing\n",
    "\n",
    "    Args:\n",
    "        class_names: list of the classification names\n",
    "    \n",
    "    Returns:\n",
    "        `tf.keras.Sequential` - a constructed tf model\n",
    "    \"\"\"\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Rescaling(1./255),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(class_names))\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractLabels(dataset: tf.data.Dataset) -> list:\n",
    "    \"\"\"\n",
    "    Take the given dataset and return a list of its labels. This can take some\n",
    "    time, try to store results into a variable when you can.\n",
    "\n",
    "    Args:\n",
    "        dataset: the dataset to extract labels from\n",
    "    \n",
    "    Returns:\n",
    "        `list[int]` - list of the labels in the dataset\n",
    "    \"\"\"\n",
    "    return list(\n",
    "        dataset.map(lambda _,y: y)\n",
    "            .flat_map(tf.data.Dataset.from_tensor_slices)\n",
    "            .as_numpy_iterator()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(class_names: list, true_labels: list, predicted_labels: list) -> None:\n",
    "    \"\"\"\n",
    "    Displays a confusion matrix for the predictions.\n",
    "\n",
    "    Args:\n",
    "        class_names: list of the classification names\n",
    "        true_labels: labels of the dataset that was tested\n",
    "        predicted_labels: list of the predictions made\n",
    "    \"\"\"\n",
    "    # Create a confusion matrix as a 2D array.\n",
    "    confusion_matrix = tf.math.confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Use a heatmap plot to display it.\n",
    "    ax = sns.heatmap(\n",
    "        confusion_matrix, \n",
    "        annot=True, \n",
    "        fmt='.3g', \n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        cbar=False\n",
    "    )\n",
    "\n",
    "    # Add axis labels.\n",
    "    ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
